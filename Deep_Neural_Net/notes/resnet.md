# 深度残差神经网络

## 摘要
随着深度神经网络的深度增加,训练难度也越来越大,这篇文章提出了一种方法,在增加网络深度的同时,不会增加网络的计算复杂度,并且增加网络的精度.

作者将网络的多个layer(2个或者3个),构建成一个关于这些layer的输入x的一个残差函数,这样整个网络变成由若干个残差块(多个普通的Layer)构成的网络,整个网络学习的就是关于这些显式定义残差函数的学习,而不是传统的非引用式学习.论文中的这个句子看了半天才明白.
> We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. 

剩下的部分,作者开始长篇大论的讲解并证明,这种使用残差块的方式与原始的方式是等价的,而且效果更好.

## 简介
首先,肯定一下神经网络的作用,在图像处理领域,特别是在图像分类,识别,检测方面都有很多突破性进展,神经网络,把提取特征(低级,中级,高级),分类等任务放在一个端到端的网络中,并且,随着网络层次的增加,提取的特征越高级.

但是神经网络存在的问题也暴露出来,就是随着网络深度的增加,梯度会消失. 这种问题,虽然可以通过在卷基层与激活函数之间增加BN来解决(或者增加正则化层).但是随着网络层次深度的增加,会出现误差增加的的现象,而且不幸的是这种问题不是由过拟合导致的.

误差曲线如下图所示
![]()

为了解决这个问题,这篇文章提出了一种残差学习的框架: 之前几个层叠的网络是直接映射(潜在的一个期望映射),这里是把这些层叠网络层进行残差映射. 比如定义之前这几个网络层的映射是 H(x),其中x是这几个网络层的输入,把这几个网络定义成残差映射就是F(x)=H(x)-x.那么原始的潜在映射就是H(x) = F(x) + x.

这个新的原始映射H(x) = F(x) + x.可以通过一个带有shortcut connection的前馈网络来实现.这里所谓的shortcut connection是跳跃几层的网络连接方式.结果如下图所示:
![]()